{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#step 1\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "# We build our custom tokenizer:\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Lowercase()\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# We can train this tokenizer by giving it a list of path to text files:\n",
        "trainer = BpeTrainer(vocab_size=1000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "files = ['tweets23_tokens.csv']\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "06rT81mp2TTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 2\n",
        "# And now it is ready, we can save the vocabulary with\n",
        "tokenizer.model.save('/content/','tokenizer-mycovid')"
      ],
      "metadata": {
        "id": "D51XArWPIoWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}